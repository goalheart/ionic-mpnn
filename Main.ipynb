{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef7946e-8d6e-4c07-8994-f1b936d0b2e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6cadae7b-0faf-45df-9753-4ffd8bd764c9",
   "metadata": {},
   "source": [
    "## 粘度预测模型（Viscosity Prediction）\n",
    "\n",
    "本代码实现了一个**基于离子对 SMILES 结构的离子液体粘度预测模型**，整体设计严格参考论文  \n",
    "**《Predicting Ionic Liquid Materials Properties from Chemical Structure》**，用于复现实验并支持后续迁移学习。\n",
    "\n",
    "---\n",
    "\n",
    "### 一、模型整体思路\n",
    "\n",
    "模型以**阳离子 / 阴离子的分子图结构**为输入，使用 **Message Passing Neural Network (MPNN)** 分别对两种离子进行编码，再将其特征进行融合，最终在**物理约束条件下**预测粘度的对数值。\n",
    "\n",
    "---\n",
    "\n",
    "### 二、核心结构\n",
    "\n",
    "- **输入**\n",
    "  - 阳离子与阴离子的：\n",
    "    - 原子类别（atom ids）\n",
    "    - 键类型（bond ids）\n",
    "    - 图连接关系（edge indices）\n",
    "  - 温度 `T`（显式作为模型输入）\n",
    "\n",
    "- **分子编码（MPNN）**\n",
    "  - 原子 / 键嵌入（Embedding）\n",
    "  - 多步消息传递（BondMatrixMessage + GatedUpdate）\n",
    "  - 全局池化（GlobalSumPool）得到分子指纹\n",
    "\n",
    "- **离子对融合**\n",
    "  - 对阳离子与阴离子指纹进行逐元素相加（paper-consistent）\n",
    "  - 经全连接层生成粘度物理参数\n",
    "\n",
    "---\n",
    "\n",
    "### 三、物理约束粘度头\n",
    "\n",
    "模型并非直接回归粘度，而是学习物理参数：\n",
    "\n",
    "\\[\n",
    "\\log(\\eta) = A + \\frac{B}{T + C}\n",
    "\\]\n",
    "\n",
    "其中：\n",
    "- \\(A\\)：无约束偏置项  \n",
    "- \\(B\\)：通过 `softplus + clip` 约束为正值  \n",
    "- \\(C\\)：保证温度平移项为合理正区间  \n",
    "- 温度 \\(T\\) 进行物理尺度归一化  \n",
    "\n",
    "该设计确保预测结果**符合实际物理规律**。\n",
    "\n",
    "---\n",
    "\n",
    "### 四、训练与评估\n",
    "\n",
    "- **数据划分**\n",
    "  - 默认采用论文中的随机划分（存在 pair-level leakage）\n",
    "  - 代码中预留了 **严格无泄漏划分（按离子对）** 的可选方案\n",
    "\n",
    "- **训练设置**\n",
    "  - Optimizer：Adam（带梯度裁剪）\n",
    "  - Loss：MSE\n",
    "  - Early Stopping 防止过拟合\n",
    "  - 自定义 Callback 控制关键 epoch 输出\n",
    "\n",
    "- **评估指标**\n",
    "  - R²（决定系数）\n",
    "  - MAE（平均绝对误差）\n",
    "\n",
    "---\n",
    "\n",
    "### 五、结果可视化与保存\n",
    "\n",
    "- 绘制训练 / 验证损失曲线\n",
    "- 生成论文对应的 **Figure 2(a)**：实验值 vs 预测值散点图\n",
    "- 保存最终模型为 `.keras` 格式，用于后续迁移学习任务\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fcfa7e71-011c-4dd9-a1bc-e851b28f17dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Cell 1: Environment setup & global config\n",
    "# - Suppress TF logs\n",
    "# - Import all core dependencies\n",
    "# ============================================\n",
    "\n",
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "from tensorflow.keras.layers import Input, Embedding, Dense, Lambda\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from models.layers import (\n",
    "    BondMatrixMessage,\n",
    "    GatedUpdate,\n",
    "    GlobalSumPool,\n",
    "    Reduce\n",
    ")\n",
    "\n",
    "EPS = 1e-6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08aa3748-7a89-46a3-b14f-fca185401e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Cell 2: Utility functions\n",
    "# - R2 metric\n",
    "# - Padding helpers\n",
    "# - Training curve plotting\n",
    "# ============================================\n",
    "\n",
    "def r2_numpy(y_true, y_pred):\n",
    "    ss_res = np.sum((y_true - y_pred) ** 2)\n",
    "    ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)\n",
    "    return 1.0 - ss_res / (ss_tot + EPS)\n",
    "\n",
    "\n",
    "def pad_sequences_1d(seq_list, max_len, pad_val=0):\n",
    "    return np.array(\n",
    "        [s + [pad_val] * (max_len - len(s)) for s in seq_list],\n",
    "        dtype=np.int32\n",
    "    )\n",
    "\n",
    "\n",
    "def plot_loss(history, out_path=\"loss_curve_viscosity.png\"):\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.plot(history.history[\"loss\"], label=\"Train loss\")\n",
    "    plt.plot(history.history[\"val_loss\"], label=\"Validation loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"MSE loss\")\n",
    "    plt.title(\"Training curve (viscosity)\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_path, dpi=300)\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18864d69-3f63-4b75-aeef-9237523ffd55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Cell 3: Graph preprocessing & custom callback\n",
    "# - Edge duplication (undirected graph)\n",
    "# - Selective verbose logging\n",
    "# ============================================\n",
    "\n",
    "def preprocess_edges_and_bonds(edge_list, bond_list, max_edges):\n",
    "    processed_edges, processed_bonds = [], []\n",
    "\n",
    "    for edges, bonds in zip(edge_list, bond_list):\n",
    "        e2, b2 = [], []\n",
    "        for (src, tgt), bond_id in zip(edges, bonds):\n",
    "            e2.append([src, tgt])\n",
    "            b2.append(bond_id)\n",
    "            e2.append([tgt, src])\n",
    "            b2.append(bond_id)\n",
    "        processed_edges.append(e2)\n",
    "        processed_bonds.append(b2)\n",
    "\n",
    "    max_len = max_edges * 2\n",
    "\n",
    "    processed_edges = [\n",
    "        e + [[0, 0]] * (max_len - len(e)) if len(e) < max_len else e[:max_len]\n",
    "        for e in processed_edges\n",
    "    ]\n",
    "    processed_bonds = [\n",
    "        b + [0] * (max_len - len(b)) if len(b) < max_len else b[:max_len]\n",
    "        for b in processed_bonds\n",
    "    ]\n",
    "\n",
    "    return (\n",
    "        np.array(processed_edges, dtype=np.int32),\n",
    "        np.array(processed_bonds, dtype=np.int32)\n",
    "    )\n",
    "\n",
    "\n",
    "class SelectiveVerboseCallback(keras.callbacks.Callback):\n",
    "    def __init__(self, total_epochs):\n",
    "        super().__init__()\n",
    "        base = [1, 2, 3, 4, 5, 50, 100, 150, 200]\n",
    "        last_five = list(range(total_epochs - 4, total_epochs + 1))\n",
    "        self.verbose_epochs = set(base + last_five)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        ep = epoch + 1\n",
    "        if ep in self.verbose_epochs:\n",
    "            print(\n",
    "                f\"Epoch {ep} - \"\n",
    "                f\"loss: {logs['loss']:.6f}, \"\n",
    "                f\"val_loss: {logs['val_loss']:.6f}\"\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e159c764-d27d-4da1-bb14-c9884ea679cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Cell 4: Physically constrained viscosity head\n",
    "# log(η) = A + B / (T + C)\n",
    "# ============================================\n",
    "\n",
    "@keras.saving.register_keras_serializable(package=\"ionic_mpnn\")\n",
    "def get_A(x):\n",
    "    return keras.ops.expand_dims(x[:, 0], -1)\n",
    "\n",
    "\n",
    "@keras.saving.register_keras_serializable(package=\"ionic_mpnn\")\n",
    "def get_B(x):\n",
    "    return keras.ops.clip(\n",
    "        keras.ops.nn.softplus(keras.ops.expand_dims(x[:, 1], -1)),\n",
    "        0.0, 20.0\n",
    "    )\n",
    "\n",
    "\n",
    "@keras.saving.register_keras_serializable(package=\"ionic_mpnn\")\n",
    "def get_C(x):\n",
    "    return keras.ops.clip(\n",
    "        keras.ops.nn.softplus(keras.ops.expand_dims(x[:, 2], -1)),\n",
    "        0.1, 50.0\n",
    "    )\n",
    "\n",
    "\n",
    "@keras.saving.register_keras_serializable(package=\"ionic_mpnn\")\n",
    "def compute_log_eta(inputs):\n",
    "    A, B, T, C = inputs\n",
    "    return A + B / (T + C + 1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "786182ec-2406-4c00-9778-abad6f2af513",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Cell 5: MPNN model definition (paper settings)\n",
    "# ============================================\n",
    "\n",
    "def build_model(\n",
    "    atom_vocab_size,\n",
    "    bond_vocab_size,\n",
    "    atom_dim=32,\n",
    "    bond_dim=8,\n",
    "    fp_size=32,\n",
    "    mixing_size=20,\n",
    "    num_steps=4\n",
    "):\n",
    "    cat_atom = Input(shape=(None,), dtype=tf.int32, name=\"cat_atom\")\n",
    "    cat_bond = Input(shape=(None,), dtype=tf.int32, name=\"cat_bond\")\n",
    "    cat_conn = Input(shape=(None, 2), dtype=tf.int32, name=\"cat_connectivity\")\n",
    "\n",
    "    an_atom = Input(shape=(None,), dtype=tf.int32, name=\"an_atom\")\n",
    "    an_bond = Input(shape=(None,), dtype=tf.int32, name=\"an_bond\")\n",
    "    an_conn = Input(shape=(None, 2), dtype=tf.int32, name=\"an_connectivity\")\n",
    "\n",
    "    T_input = Input(shape=(1,), dtype=tf.float32, name=\"temperature\")\n",
    "\n",
    "    atom_emb = Embedding(atom_vocab_size, atom_dim)\n",
    "    bond_emb = Embedding(bond_vocab_size, bond_dim)\n",
    "\n",
    "    def encode(atom_ids, bond_ids, conn, prefix):\n",
    "        h = atom_emb(atom_ids)\n",
    "        b = bond_emb(bond_ids)\n",
    "\n",
    "        for i in range(num_steps):\n",
    "            m = BondMatrixMessage(atom_dim, bond_dim)([h, b, conn])\n",
    "            agg = Reduce()([m, conn[:, :, 1], h])\n",
    "            h = GatedUpdate(atom_dim)([h, agg])\n",
    "\n",
    "        fp = GlobalSumPool()([h, atom_ids])\n",
    "        fp = Dense(fp_size, activation=\"relu\", kernel_regularizer=l2(1e-5))(fp)\n",
    "        return fp\n",
    "\n",
    "    fp_cat = encode(cat_atom, cat_bond, cat_conn, \"cat\")\n",
    "    fp_an  = encode(an_atom,  an_bond,  an_conn,  \"an\")\n",
    "\n",
    "    mixed = Dense(mixing_size, activation=\"relu\")(fp_cat + fp_an)\n",
    "\n",
    "    params = Dense(3)(mixed)\n",
    "    A = Lambda(get_A)(params)\n",
    "    B = Lambda(get_B)(params)\n",
    "    C = Lambda(get_C)(params)\n",
    "\n",
    "    T_scaled = Lambda(lambda t: t / 100.0)(T_input)\n",
    "    log_eta = Lambda(compute_log_eta)([A, B, T_scaled, C])\n",
    "\n",
    "    model = Model(\n",
    "        inputs=[\n",
    "            cat_atom, cat_bond, cat_conn,\n",
    "            an_atom, an_bond, an_conn,\n",
    "            T_input\n",
    "        ],\n",
    "        outputs=log_eta\n",
    "    )\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(1e-3, clipnorm=1.0),\n",
    "        loss=\"mse\"\n",
    "    )\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7f10f70-e0e1-4a18-a11f-cdb40ebbf955",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1765700526.283727  962109 service.cc:145] XLA service 0x77e27000bf40 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1765700526.283795  962109 service.cc:153]   StreamExecutor device (0): NVIDIA GeForce RTX 4090, Compute Capability 8.9\n",
      "I0000 00:00:1765700549.015312  962109 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - loss: 97.266747, val_loss: 12.758076\n",
      "Epoch 2 - loss: 11.275120, val_loss: 14.831838\n",
      "Epoch 3 - loss: 7.950568, val_loss: 15.828735\n",
      "Epoch 4 - loss: 3.897224, val_loss: 0.671229\n",
      "Epoch 5 - loss: 3.212166, val_loss: 3.792301\n",
      "Epoch 50 - loss: 0.122934, val_loss: 0.153441\n",
      "Epoch 100 - loss: 0.090418, val_loss: 0.104600\n",
      "Epoch 150 - loss: 0.065704, val_loss: 0.071188\n",
      "Epoch 200 - loss: 0.055696, val_loss: 0.081959\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 56ms/step\n",
      "Train R2: 0.9235160742399561\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 422ms/step\n",
      "Dev   R2: 0.8889337277972653\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step \n",
      "Test  R2: 0.8737756648689126\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Cell 6: Data loading, training, evaluation\n",
    "# ============================================\n",
    "\n",
    "DATA = \"data/viscosity_id_data.pkl\"\n",
    "VOCAB = \"data/vocab.pkl\"\n",
    "\n",
    "with open(DATA, \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "with open(VOCAB, \"rb\") as f:\n",
    "    vocab = pickle.load(f)\n",
    "\n",
    "atom_vocab_size = vocab[\"atom_vocab_size\"] + 1\n",
    "bond_vocab_size = vocab[\"bond_vocab_size\"] + 1\n",
    "\n",
    "pair_ids = [d[\"pair_id\"] for d in data]\n",
    "\n",
    "cat_atoms = [[a + 1 for a in d[\"cation\"][\"atom_ids\"]] for d in data]\n",
    "cat_bonds = [[b + 1 for b in d[\"cation\"][\"bond_ids\"]] for d in data]\n",
    "cat_edges = [d[\"cation\"][\"edge_indices\"] for d in data]\n",
    "\n",
    "an_atoms  = [[a + 1 for a in d[\"anion\"][\"atom_ids\"]] for d in data]\n",
    "an_bonds  = [[b + 1 for b in d[\"anion\"][\"bond_ids\"]] for d in data]\n",
    "an_edges  = [d[\"anion\"][\"edge_indices\"] for d in data]\n",
    "\n",
    "T = np.array([d[\"T\"] for d in data], np.float32)[:, None]\n",
    "y = np.array([d[\"log_eta\"] for d in data], np.float32)\n",
    "\n",
    "idx = np.arange(len(data))\n",
    "idx_train, idx_tmp = train_test_split(idx, test_size=0.30, random_state=42)\n",
    "idx_dev, idx_test  = train_test_split(idx_tmp, test_size=0.50, random_state=42)\n",
    "\n",
    "max_atoms = max(max(map(len, cat_atoms)), max(map(len, an_atoms)))\n",
    "max_edges = max(max(map(len, cat_edges)), max(map(len, an_edges)))\n",
    "\n",
    "def build_inputs(idxs):\n",
    "    ce, cb = preprocess_edges_and_bonds(\n",
    "        [cat_edges[i] for i in idxs],\n",
    "        [cat_bonds[i] for i in idxs],\n",
    "        max_edges\n",
    "    )\n",
    "    ae, ab = preprocess_edges_and_bonds(\n",
    "        [an_edges[i] for i in idxs],\n",
    "        [an_bonds[i] for i in idxs],\n",
    "        max_edges\n",
    "    )\n",
    "    return {\n",
    "        \"cat_atom\": pad_sequences_1d([cat_atoms[i] for i in idxs], max_atoms),\n",
    "        \"cat_bond\": cb,\n",
    "        \"cat_connectivity\": ce,\n",
    "        \"an_atom\": pad_sequences_1d([an_atoms[i] for i in idxs], max_atoms),\n",
    "        \"an_bond\": ab,\n",
    "        \"an_connectivity\": ae,\n",
    "        \"temperature\": T[idxs]\n",
    "    }\n",
    "\n",
    "x_train, x_dev, x_test = map(build_inputs, [idx_train, idx_dev, idx_test])\n",
    "y_train, y_dev, y_test = y[idx_train], y[idx_dev], y[idx_test]\n",
    "\n",
    "model = build_model(atom_vocab_size, bond_vocab_size)\n",
    "\n",
    "history = model.fit(\n",
    "    x_train, y_train,\n",
    "    validation_data=(x_dev, y_dev),\n",
    "    epochs=1000,\n",
    "    batch_size=32,\n",
    "    callbacks=[\n",
    "        EarlyStopping(patience=50, restore_best_weights=True),\n",
    "        SelectiveVerboseCallback(total_epochs=1000)\n",
    "    ],\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "model.save(\"models/viscosity_final.keras\")\n",
    "\n",
    "print(\"Train R2:\", r2_numpy(y_train, model.predict(x_train).flatten()))\n",
    "print(\"Dev   R2:\", r2_numpy(y_dev,   model.predict(x_dev).flatten()))\n",
    "print(\"Test  R2:\", r2_numpy(y_test,  model.predict(x_test).flatten()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "31d7f989-727f-41d9-84b9-d0f3e22bcaa4",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid or missing encoding declaration for 'results/figure2_a_viscosity.png' (<string>)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "\u001b[0;36m  File \u001b[0;32m~/miniconda3/envs/ionic-mpnn/lib/python3.9/tokenize.py:330\u001b[0;36m in \u001b[0;35mfind_cookie\u001b[0;36m\n\u001b[0;31m    line_string = line.decode('utf-8')\u001b[0;36m\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m\u001b[0;31m:\u001b[0m 'utf-8' codec can't decode byte 0x89 in position 0: invalid start byte\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "\u001b[0m  File \u001b[1;32m~/miniconda3/envs/ionic-mpnn/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3526\u001b[0m in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\u001b[0m\n",
      "\u001b[0m  Cell \u001b[1;32mIn[7], line 1\u001b[0m\n    get_ipython().run_line_magic('load', './results/figure2_a_viscosity.png')\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32m~/miniconda3/envs/ionic-mpnn/lib/python3.9/site-packages/IPython/core/interactiveshell.py:2432\u001b[0m in \u001b[1;35mrun_line_magic\u001b[0m\n    result = fn(*args, **kwargs)\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32m~/miniconda3/envs/ionic-mpnn/lib/python3.9/site-packages/IPython/core/magics/code.py:359\u001b[0m in \u001b[1;35mload\u001b[0m\n    contents = self.shell.find_user_code(args, search_ns=search_ns)\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32m~/miniconda3/envs/ionic-mpnn/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3834\u001b[0m in \u001b[1;35mfind_user_code\u001b[0m\n    return openpy.read_py_file(tgt, skip_encoding_cookie=skip_encoding_cookie)\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32m~/miniconda3/envs/ionic-mpnn/lib/python3.9/site-packages/IPython/utils/openpy.py:77\u001b[0m in \u001b[1;35mread_py_file\u001b[0m\n    with open(filepath) as f:  # the open function defined in this module.\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32m~/miniconda3/envs/ionic-mpnn/lib/python3.9/tokenize.py:394\u001b[0m in \u001b[1;35mopen\u001b[0m\n    encoding, lines = detect_encoding(buffer.readline)\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32m~/miniconda3/envs/ionic-mpnn/lib/python3.9/tokenize.py:371\u001b[0m in \u001b[1;35mdetect_encoding\u001b[0m\n    encoding = find_cookie(first)\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m~/miniconda3/envs/ionic-mpnn/lib/python3.9/tokenize.py:335\u001b[0;36m in \u001b[0;35mfind_cookie\u001b[0;36m\n\u001b[0;31m    raise SyntaxError(msg)\u001b[0;36m\n",
      "\u001b[0;36m  File \u001b[0;32m<string>\u001b[0;36m\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid or missing encoding declaration for 'results/figure2_a_viscosity.png'\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41fc6fe-2605-4dd7-99e0-733d762026fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
